{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e77478a",
   "metadata": {},
   "source": [
    "# Probabilistic Programming\n",
    "\n",
    "In this notebook we'll take a look at an approach for solving problems that have a great deal of *uncertainty*.  That is - we want to make use of the fact that we can quantify what we do and do not know.  We can write programs that take this uncertainty into account.  This is known as _probabilistic programming_.\n",
    "\n",
    "The first step is to assume that there is a process that _generates_ the data (i.e., this is a generative model).  As such, this process has a _functional form_ and some _parameters_.  \n",
    "\n",
    "Second, we apply *Bayes' Rule*:\n",
    "\n",
    "$$ P(Parameters | Data) = \\frac {P(Data | Parameters) \\cdot P(Parameters)} {P(Data)} $$\n",
    "\n",
    "In simpler terms:  In order to get values for the parameters (left-hand side), we need to:\n",
    "*  Find the values of the parameters that are most _likely_ for the data we're seeing (left term in the numerator)\n",
    "*  Assume how parameters are distributed (right term in the numerator)\n",
    "*  Assume how data is distributed (denominator)\n",
    "\n",
    "This can get very complicated very quickly, so we use _Monte-Carlo methods_:  These are statistical methods that rely on _random sampling_ of thousands or millions of values, and then seeing how likely these values are.  By combining these estimates we can then measure how certain/uncertain our results are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3287e6",
   "metadata": {},
   "source": [
    "# Motivating Example\n",
    "\n",
    "Assume we have 3 speakers from our organization who present at a conference.  Attendees can indicate (maybe using a web app) whether they liked the talk or not.  Who is the best speaker?\n",
    "\n",
    "Let's bring in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39502f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import pandas as pd     # Dataframe library\n",
    "import pymc3 as pm      # Markov-Chain Monte-Carlo (MCMC) library\n",
    "import arviz as az      # Visualization for MCMC libraries\n",
    "import matplotlib       # Basic visualization library\n",
    "\n",
    "# Default figure size\n",
    "matplotlib.rcParams['figure.figsize'] = (20,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speakers import get_data\n",
    "\n",
    "df = get_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of likes and unlikes\n",
    "\n",
    "df_counts = df.groupby(['name', 'value']).size().unstack()\n",
    "df_counts.columns = ['unlike','like']\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25070b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add average value for each speaker\n",
    "\n",
    "df_counts['total'] = df_counts['like'] + df_counts['unlike']\n",
    "df_counts['avg'] = df_counts['like'] / df_counts['total']\n",
    "\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b10344",
   "metadata": {},
   "source": [
    "According to this, Yaniv is the best speaker while Yuval is the worst.  Is this a good way to analyse the data?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334b2c2",
   "metadata": {},
   "source": [
    "## Math Detour\n",
    "\n",
    "Assume we have some data, and would like to understand the process which created it.  That process has a *functional form* and some *parameters*.  This is called a *distribution*.  Let's explain.\n",
    "\n",
    "### Statistical Distributions\n",
    "\n",
    "\n",
    "A *statistical distribution* is simply a function that assigns a non-negative probability value to its inputs.  The sum of all values should be exactly equal to 1.0.  The inputs are known as a _sample space_ and can be anything we'd like.  \n",
    "\n",
    "#### Example 1\n",
    "\n",
    "Let's look at the well-known example of a coin toss.  The possible inputs - or sample space - are simply the sides of the coin.  The distribution will then assign each a value:\n",
    "\n",
    "Heads --->   0.5\n",
    "\n",
    "Tails --->   0.5\n",
    "\n",
    "This is a fair coin, since the probability of both sides are equal.  If we toss the coin 100 times, we expect - on average - to get 50 heads and 50 tails.\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "We can also have an _unfair_ (or biased) coin:\n",
    "\n",
    "Heads ---> 0.2\n",
    "\n",
    "Tails ---> 0.8\n",
    "\n",
    "If we toss the coin 100 times we expect - on average - the coin to come up heads 20 times and tails 80 times.\n",
    "\n",
    "\n",
    "### Bernoulli Distribution\n",
    "\n",
    "In the previous examples we see a pattern:  sample space composed of two values where one has the probability $p$ and the other the probability $1-p$.  This is known as the _Bernoulli Distribution_ and has the functional form:\n",
    "\n",
    "$$ P(X=x) = p^{x} \\cdot (1-p)^{(1-x)} $$\n",
    "\n",
    "Its intuitive explanation is as follows:  we run a single experiment which has two possible outputs.  We call one output '1' and the other '0'.  The probability for getting 1 is $p$ and the probability for getting 0 is $1-p$.\n",
    "\n",
    "#### Example 3\n",
    "\n",
    "Assume a regular 6-sided playing die.  Its sample space and associated values:\n",
    "\n",
    "1 ---> 1/6\n",
    "\n",
    "2 ---> 1/6\n",
    "\n",
    "3 ---> 1/6\n",
    "\n",
    "4 ---> 1/6\n",
    "\n",
    "5 ---> 1/6\n",
    "\n",
    "6 ---> 1/6\n",
    "\n",
    "\n",
    "### Uniform Distribution \n",
    "\n",
    "When you have $n$ possible outcomes and each outcome is equally likely, you have a _uniform distribution_.  Its functional form:\n",
    "\n",
    "$$ P(X=x) = \\frac{1}{n} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa05bec",
   "metadata": {},
   "source": [
    "## Back to our scheduled broadcast...\n",
    "\n",
    "Using averages is probably not the best idea:\n",
    "*  Different amounts of data for each speaker\n",
    "*  Each speaker talks about a different subject - how can we compare?\n",
    "*  Different people in each talk, with different interests and conditions - how can we compare?\n",
    "\n",
    "Instead, let's try a different approach.  Assume each speaker has a _probability of getting a like_.  This includes:\n",
    "*  Their ability to choose an interesting subject\n",
    "*  Their ability to prepare a good presentation \n",
    "*  Their ability to deliver the presentation in an engaging manner,\n",
    "*  More...\n",
    "\n",
    "Why is it a probability?  Because there are different people that listen to each talk.  They may or may not like the presentation or delivery, may or may not be bothered by other things, etc.  Essentially, we are allowing for some _uncertainty_ to come into play.  \n",
    "\n",
    "So we can the following:  The process that generates the likes for each speaker is based on n (the number of people) different 'experiments' where the chance of success is unique to each speaker.\n",
    "\n",
    "In statistical terms:  Our data is Bernoulli-distributed, with a parameter $p$ that depends on the speaker.  We assume that any value of $p$ between 0 and 1 can be possible.\n",
    "\n",
    "Let's write the code and find the value of $p$ that is most likely to have generated the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple speakers - we need to assign each observation ('experiment result') to a specific speaker\n",
    "name_idx, names = pd.factorize(df['name'])\n",
    "name_idx, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aeea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model!!!\n",
    "\n",
    "with pm.Model() as single_value:\n",
    "    \n",
    "    # The 'like probability' - between 0 and 1, assume there is a single value for all speakers\n",
    "    like_probability = pm.Uniform(\"like_prob\", lower=0.0, upper=1.0)\n",
    "    \n",
    "    # The generating process, with the prob. of likes and the actual data for each speaker\n",
    "    likes = pm.Bernoulli(\"likes\", p=like_probability, observed=df['value'][name_idx])\n",
    "    \n",
    "    # Sample 5000 points * 4 chains of possible like_probability values and see which describes the data better\n",
    "    trace = pm.sample(5000, chains=4, return_inferencedata=False)\n",
    "    \n",
    "    # Summary statistics and plots\n",
    "    print(az.summary(trace))\n",
    "    az.plot_trace(trace)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a4c14",
   "metadata": {},
   "source": [
    "Does the mean value of like_prob seem reasonable?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da3819",
   "metadata": {},
   "source": [
    "## Another Math Detour (WHAT????)\n",
    "\n",
    "### The Beta Distribution\n",
    "\n",
    "You've probably come across the Bernoulli and Uniform distributions before.  And even if not - they're pretty intuitive.  Let's see another one, named the _Beta_ distribution.\n",
    "\n",
    "First off, we won't show the functional form, as it's not really relevant.  What's important to understand is that in the Beta distribution, our sample space consists of all possible values between 0 and 1.  So there is a probability value for 0.0, 0.001, 0.393984, 0.2994774 and any other number between 0 and 1.  \n",
    "\n",
    "This is interesting because it allows us to give a probability to our 'like_prob' variable!   HUH???\n",
    "\n",
    "In our last model, we assumed nothing about the value of like_probability.  Or rather, we assumed all values are equally likely, from 0.0 to 1.0.  But is that right?  Conferences to bring in good speakers and people tend to go see speakers they're familiar with.  So an experienced speaker will likely have a like_probability in the upper range of 0 to 1.  We can use the Beta to model this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible values of sample space\n",
    "xs = np.arange(0,1.01,0.01)\n",
    "\n",
    "# Generate a Beta that is centered on 0.8 and has a sd of 0.1\n",
    "beta_dist = pm.Beta.dist(mu=0.8, sd=0.1)\n",
    "\n",
    "# Plot\n",
    "plot(xs, np.exp(beta_dist.logp(xs).eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd53d73",
   "metadata": {},
   "source": [
    "So now we have some better assumptions on how like_probility behaves.  This is called a _prior_.  We can simply plug it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfa0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as single_value_with_prior:\n",
    "    \n",
    "    # Like_probability is now assumed to have a Beta distribution prior\n",
    "    like_probability = pm.Beta(\"like_prob\", mu=0.8, sd=0.1)\n",
    "    \n",
    "    likes = pm.Bernoulli(\"likes\", p=like_probability, observed=df['value'][name_idx])\n",
    "    trace = pm.sample(5000, chains=4, return_inferencedata=False)\n",
    "    \n",
    "    print(az.summary(trace))\n",
    "    az.plot_trace(trace)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab529e",
   "metadata": {},
   "source": [
    "Does this value make sense?  What about the fact that we have a single value for 3 speakers who may be talking about wildly different things?  Let's account for that in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eddeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How many speakers do we have?\n",
    "num_names = len(names)\n",
    "\n",
    "with pm.Model() as multiple_values:\n",
    "    \n",
    "    # Still Beta-distributed, but now we have a 'shape' parameter with multiple such values\n",
    "    like_probability = pm.Beta(\"like_prob\", mu=0.8, sd=0.1, shape=num_names)\n",
    "    \n",
    "    # The Bernoulli parameter p now has an index that refers to the speaker-specific value\n",
    "    likes = pm.Bernoulli(\"likes\", p=like_probability[name_idx], observed=df['value'][name_idx])\n",
    "    \n",
    "    trace = pm.sample(5000, chains=4, return_inferencedata=False)\n",
    "    \n",
    "    print(az.summary(trace))\n",
    "    az.plot_trace(trace)\n",
    "    \n",
    "    # Show a comparison of all speakers\n",
    "    az.plot_forest([trace['like_prob'][:,i] for i in range(num_names)], combined=True, model_names=names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddda37",
   "metadata": {},
   "source": [
    "We now have 3 values that are closer to what we assumed.  They are also more in-line with our experience as speakers.  Not only that, but it's no longer clear who the best speaker is, as all three are close.  \n",
    "\n",
    "\n",
    "We still have another problem to deal with:  there are different amounts of data for each speaker.  That will depend on the subject of each talk - so speakers with less 'mainstream' interests will receive less results (likes or unlikes).  Is there something we can do?\n",
    "\n",
    "Yes, there is!  In the previous model we assumed that the distribution of like_probability was identical for each speaker but independent - i.e., there is no relation between like_probability of one speaker to that of another.  But we know that all 3 speakers were trained by the same organization.  So maybe what we should do is parameterize the distribution of like_probability itself.  That is, the values for each speaker have their own distribution (or 'hyper-distribution').  This is known as a _hierarchical model_ and is extremely powerful.  In essence, what it allows us to do is to 'transfer' knowledge from a speaker with more attendees to one with less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pm.Model() as hierarchical_multiple_values:\n",
    "    \n",
    "    # Assume that the average like_probability for all our speakers is in the range 0.65 to 0.85\n",
    "    like_mean = pm.Uniform(\"like_mean\", lower=0.65, upper=0.85)\n",
    "    \n",
    "    # Bring that assumption in\n",
    "    like_probability = pm.Beta(\"like_prob\", mu=like_mean, sd=0.05, shape=num_names)\n",
    "    \n",
    "    likes = pm.Bernoulli(\"likes\", p=like_probability[name_idx], observed=df['value'][name_idx])\n",
    "    \n",
    "    trace = pm.sample(5000, return_inferencedata=False)\n",
    "    \n",
    "    print(az.summary(trace))\n",
    "    az.plot_trace(trace)\n",
    "    az.plot_forest([trace['like_prob'][:,i] for i in range(num_names)], combined=True, model_names=names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263b0d2",
   "metadata": {},
   "source": [
    "So what about these values?\n",
    "\n",
    "We can build bigger and bigger hierarchical models that will take more and more sources of knowledge into account.  But now it is time to compare our initial results to what we've accomplished.  Is the comparison clear?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859292f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean value like_prob for each speaker\n",
    "df_counts['mcmc'] = np.array([trace['like_prob'][:,i].mean() for i in range(num_names)]).T\n",
    "\n",
    "df_counts[['avg','mcmc']].plot()\n",
    "df_counts\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
