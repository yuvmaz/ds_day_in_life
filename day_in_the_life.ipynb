{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Data Scientist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _data scientist_ answers questions about _data_.  Given some data and a _business question_, the DS gives a quantitive response.  Some questions that a DS can answer:\n",
    "\n",
    "* How many units are we going to sell in the next quarter?\n",
    "* What is our conversion rate for 3, 6 and 12 months?\n",
    "* Which of our machines will fail this year?\n",
    "\n",
    "Data Science is a combination of the following:\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/993/1*jd_xgOzAFw7rBklREtgOTQ.jpeg\" width=400 height=400 />\n",
    "\n",
    "\n",
    "In essence, data science is about _telling a story_ to stakeholders.  The story should allow stakeholders to understand the reasoning behind the answer and where it comes from.  A good data scientist tells great stories...\n",
    "\n",
    "[TEDx - The Magical Science of Storytelling](https://www.youtube.com/watch?v=Nj-hdQMa3uA)\n",
    "\n",
    "The above notwithstanding, in this talk we'll do something different.  Rather than a well-defined business question - which is critical in a good data science project - we'll look at some data and do what is known as *Exploratory Data Analysis*.  That is, we'll get a feeling for some data and see what questions can be relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Data for Today\n",
    "\n",
    "Our data consists of metadata (that is - data _about_ data) for a well-known political page on Facebook.  Our _Data Engineer_ setup a pipeline for scraping and initial pre-processing, so we get new data every week.  It's now time for us to get to work.\n",
    "\n",
    "The tools we'll be using:\n",
    "* Python - The swiss-army knife of the Internet.  Has libraries for pretty much anything you can think of.  It has an exceptionally good data-science set of tools.\n",
    "* Pandas - A library for working with data frames.  A data frame is very similiar to an Excel spreadsheet - data is organized in lines and columns.  We can then do a lot of interesting things with this data.\n",
    "* Numpy - A set of highly-optimized routines for dealing with arrays of numbers.  \n",
    "* Matplotlib - A set of basic primitives for doing plots and charts\n",
    "* Seaborn - A library for doing common statistical plots\n",
    "* Statsmodels - A library for fitting regression model\n",
    "* pymc3 - A library for runing Markov-Chain Monte-Carlo (MCMC) simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up and Preparing the Data for Analysis\n",
    "\n",
    "Real-life data is often messy and un-organized.  We need to make sure that it's \"clean\" and relevant in order for us to get to work.  Luckily, our data engineer is a good sport and gives us CSV files.  Let's get started!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load numpy and friends\n",
    "%pylab inline   \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import pymc3 as pm\n",
    "\n",
    "# Set some defaults\n",
    "rcParams['figure.figsize'] = (20,7)\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.csv')\n",
    "\n",
    "print(\"Length of data: \", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the following kind of data:\n",
    "* Time values (post_time, fetched_time)\n",
    "* Numerical values\n",
    "* Boolean \n",
    "\n",
    "Is this what pandas thinks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems well, except the time fields, which are treated as generic objects.  Let's change that so that we can apply time functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.post_time = pd.to_datetime(df.post_time, dayfirst=True)\n",
    "df.fetched_time = pd.to_datetime(df.fetched_time, dayfirst=True)\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!  Now let's look at the numerical values - anything out of range?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good.  Note also the 'count' field - this means that there are no missing values.  Real data often does have some.\n",
    "\n",
    "How about the non-numerical fields?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['post_time'].describe(), \"\\n\")\n",
    "print(df['fetched_time'].describe(), \"\\n\")\n",
    "print(df['has_image'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, looks like there is a issue with the `fetched_time` field.  There are only 714 values but our data has 2849 values!  Seems like a bug, as `fetched_time` is data that our scraping software adds.  We'll alert our data engineer colleauge but in the meantime - what do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where we need to think.  Do we -\n",
    "* Remove this field?\n",
    "* 'Fill it in' somehow?  (The technical term is 'imputate')\n",
    "\n",
    "Since `fetched_time` is just a technical field that we added, how about we just fill in the values with the latest available `fetched_time`?  This should be a good approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_time = df.fetched_time.max()\n",
    "df['fetched_time'].fillna(latest_time, inplace=True)\n",
    "\n",
    "df['fetched_time'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there.  Just another small issue:  we're dealing with reactions to Facebook posts.  Obviously, this is time-dependent in the sense that posts stay 'fresh' for a while and keep on getting likes, shares and so on.  So if we catch a post while it's still 'fresh', it might not have gotten everything it could get.\n",
    "\n",
    "Ideally, we could model how long a post is considered fresh for this page and ignore those.  But we only have total reactions and not the specific times.  So we make assumptions.  Let's a post stays fresh for 48 hours since users may not enter Facebook every single day and might react with a certain time delay.  \n",
    "\n",
    "How can we tell what posts are fresh?  We compare the `fetched_time` and `post_time` fields.  Let's add an `age` field and `is_fresh` field to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['fetched_time'] - df['post_time']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = (df['fetched_time'] - df['post_time']).apply(lambda delta: delta.days )\n",
    "df['is_fresh'] = df['age'] < 2\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create another dataframe which includes only 'stale' posts which have reached a stable state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stale = df[df.is_fresh == False].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to go!  Now we can start having fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Know the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first question that comes to mind is what does the distribution of post ages look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersting.  It seems that the volume of activity has increased over the last 6 months of so.  Before that there also seem to be periods of more or less activity but not as intense. \n",
    "\n",
    "Let's see if we can find some correlations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = ['post_time','fetched_time']\n",
    "bool_columns = ['has_image', 'is_fresh']\n",
    "\n",
    "def hide_current_axis(*args, **kwds):\n",
    "    gca().set_visible(False)\n",
    "    \n",
    "    \n",
    "numerical_columns = [col for col in df.columns if col not in time_columns + bool_columns]\n",
    "pair_plot = sns.pairplot(df_stale[numerical_columns])\n",
    "pair_plot.map_upper(hide_current_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There don't seem to be any super-obvious correlations between numerical metrics of a post.  There might be some connections between the the number of `likes` and `sorry`, `anger`, and `love`.  That makes sense, as these are all expressions of emotion.  \n",
    "\n",
    "But it does look like there's some sort of pattern between `age` and `likes`, `comments` and `shares`.  Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vars = ['likes','comments','shares']\n",
    "\n",
    "fig, axs = subplots(3,1, sharex=True, figsize=(20,12))\n",
    "for y_var, this_ax in zip(y_vars, axs):\n",
    "    this_ax.scatter(df_stale['age'], df_stale[y_var], alpha=0.3)\n",
    "    this_ax.set_ylabel(y_var)\n",
    "    \n",
    "fig.tight_layout(pad=3)\n",
    "xlabel(\"Post Age (in days)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, definitely something going on in the last 6 months or so.  It seems that the minimum number of likes for posts have increased.  There also seems to be a change in the amounts of comments and maybe a small change in the amount of shares.  Could this be a cycle where the author is inspired to write more because of responses which in turn bring more responses and so on?\n",
    "\n",
    "Let's look at the how long each post was published after the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stale['time_since_previous'] = (df_stale['post_time'] - df_stale['post_time'].shift(-1)) \\\n",
    "    .apply(lambda x: x.seconds / (60 * 60)).fillna(0)\n",
    "\n",
    "# What was the date 180 days ago?\n",
    "date_6_months_ago = df_stale['post_time'].max()- pd.Timedelta(days=180)\n",
    "\n",
    "# How many posts were published since then?\n",
    "num_posts_last_6_months = len(df_stale[df_stale['post_time'] >= date_6_months_ago])\n",
    "\n",
    "\n",
    "ax = subplot()\n",
    "df_stale['time_since_previous'].plot(ax=ax, zorder=2)\n",
    "ax.set_xlabel(\"Number of posts ago\")\n",
    "ax.set_ylabel(\"Time Since Previous Post (hours)\");\n",
    "ax.vlines(num_posts_last_6_months, 0, df_stale['time_since_previous'].max(), 'red', '--', zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's probably less than 6 months. There are additional techniques that can help us figure out exactly when, but we won't deal with them here.\n",
    "\n",
    "Let's look at something else.  Does having an image make a difference in the amount of responses to a post?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stale['total_responses'] = 0\n",
    "\n",
    "# Just 'likes' and similiar - is this right?\n",
    "for col in ['likes', 'love', 'support', 'wow', 'haha', 'anger', 'sorry']:\n",
    "    df_stale['total_responses'] += df_stale[col]\n",
    "    \n",
    "    \n",
    "scatter(df_stale['has_image'], df_stale['total_responses'], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of hard to tell whether adding an image to the post makes a difference.  Intuition is that it does - so this is exactly the place to apply machine learning techniques and get an exact answer.  We'll apply a tool called _linear regression_ which will give us the probability that this is indeed so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_model = smf.ols('total_responses ~ has_image', data=df_stale).fit()\n",
    "linear_regression_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of metrics, most of which are irrelevant for us right now.  What we are interested in is the second table. It tells us that a post without images gets *on average* 3120 responses and an image adds (again, on average) 1520 additional responses.  It is also worthwhile looking at how confident the model is about these results, although we will not do that here.  The effect is positive, although it may not be very big.  Let's see this in graphical form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, slope = linear_regression_model.params\n",
    "\n",
    "scatter(df_stale['has_image'], df_stale['total_responses'], alpha=0.3)\n",
    "xs = np.arange(0,1.01,0.01)\n",
    "plot(xs, intercept + slope*xs, linewidth=3, label='effect')\n",
    "plot(xs, intercept + 0*xs, 'r--', linewidth=1, label='constant line')\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at yet another characteristic of the data.  What is the median number of responses we get when we post at a given day of week/hour?  We use the median because there is a lot of variance in the data which the mean (average) is sensitive to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down 'time' into day of way and hour\n",
    "df_temp = df_stale[['post_time','total_responses']].copy()\n",
    "df_temp['post_day'] = df_temp['post_time'].apply(lambda t : (t.dayofweek + 1) % 7) # Adjust for Sunday == 0\n",
    "df_temp['post_hour'] = df_temp['post_time'].apply(lambda t : t.hour)\n",
    "\n",
    "# Group by DOW and hour and get median number of responses\n",
    "df_temp = df_temp.groupby(['post_day', 'post_hour']).median().reset_index()\n",
    "df_temp.index = pd.MultiIndex.from_frame(df_temp[['post_day','post_hour']])\n",
    "\n",
    "# Build grid of data\n",
    "grid = np.ones((7,24)) * -2000\n",
    "for ((day, hour), row) in df_temp.iterrows():\n",
    "    grid[day,hour] = row['total_responses']\n",
    "    \n",
    "# Show grid graphically - Monday is 0, Sunday is 6\n",
    "imshow(grid, cmap='magma')\n",
    "colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this heatmap we can clearly see the areas where we don't have any data (black).  It would also appear that a lot of activity happens on posts that are submitted either early in the morning (earlier than 9AM) or late in the evening (later than 9PM).\n",
    "\n",
    "Let's also look at the connection between number of words and the amount of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(df_stale['words'], df_stale['total_responses'], alpha=0.8)\n",
    "vlines(142, 0, 50000, 'red', '--', linewidth=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obvious patterns, except maybe for the fact that the author does not like posts that are longer than around 140 words.  Let's look at a histogram of post lengths, along with their median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = sns.distplot(df_stale['words'], kde=False, rug=True, bins=50)\n",
    "vlines(df_stale['words'].median(), 0, axes.get_ylim()[1], 'red', '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's clear the author prefers short posts - most posts are under 50 words long.  The histogram has a peculiar shape though - if we can find something that gives us similiar histograms we might be able to get some insights into what's going on.\n",
    "\n",
    "For fitting this *distribution* of data, we'll apply a simulation technique called Markov-Chain Monte Carlo.  Essentially what we'll try to do is describe the process that *generates* this data (which is why this is known as *generative* model) by using statistical objects known as *probability distributions*.  If our simulation is good, we'll be able to generate fake data that appears similiar to the real data.  We'll then be able to reason about what exactly is going on.  For this model we'll use a distribution known as the *negative binomial* and try to estimate its two parameters, $ \\alpha $ and $ \\mu $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as post_length_model:\n",
    "    alpha = pm.Normal('alpha', 45, 5)  \n",
    "    mu = pm.Normal('mu', 2, 2)\n",
    "    words = pm.NegativeBinomial('words', alpha, mu, observed=df_stale['words'])\n",
    "    trace = pm.sample(2000)\n",
    "    \n",
    "print(pm.summary(trace))\n",
    "pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots mean that  our simulation has converged - $ \\alpha $ appears to have a value between 45 and 48 while $ mu $ has a value between 1.35 and 1.50 (left side plots).  We can also see that the simulation tried many values in this range (right side plots).  \n",
    "\n",
    "Given these parameters, let's try to use the negative binomial to create ('generate') fake data.  We'll then plot this data against the real data and see if it looks similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pm.NegativeBinomial.dist(trace['alpha'].mean(), trace['mu'].mean()).random(size=(4, len(df_stale)))\n",
    "\n",
    "fig, axs = subplots(2,2, sharex=True, sharey=True,squeeze=True)\n",
    "\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    sns.distplot(fake_data[idx], ax=ax, bins=50, color='red', hist_kws={'alpha': 0.4})\n",
    "    sns.distplot(df_stale['words'], ax=ax, bins=50, color='green', hist_kws={'alpha': 0.4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our simulation gave us a good approximation!  The functional form appears similiar even if the details are not identical.  We can probably safely try to analyze our author's behavior by thinking about what happens with negative binomial data.  In general, the negative binomial is a *mixture model* - it means that there are many instances of the same phenomenon, but with different parameters.  These parameters, however, are not random but also have rules as to how they are selected.  For a simple example, think of a road on which only cars of the same make and model (for instance, a 2019 Hyundai Ioniq) drive but with a speed that is between 80 and 110 km/h.  \n",
    "\n",
    "When applying this kind of process to our data, we can think of 2 different theories:\n",
    "\n",
    "1.  The author has many ideas, and different opinions on each idea.  Each such opinion has a rule ('distribution') on the number of words it requires to express.  Now imagine that our author randomly chooses how many posts they will post on a given day according to some other rule and what idea will be in each post.  Therefore, each post will then comply with its specific set of possible lengths.\n",
    "2.  A different interpreation of the data may include *multiple* authors, each with their own style of writing.  Every day, a random number of authors are chosen by some rule to write a number of posts.  Each post then has the length that relates to its author's style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this talk we saw some of the work that a data scientist performs.  In particular, we saw examples of:\n",
    "\n",
    "* Data cleanup and imputation\n",
    "* The process of coming up with a theory and then proving/disproving it, using data aggregation and modelling\n",
    "* Some python tools for doing the above\n",
    "\n",
    "For this session we only focused on doing some exploratory work on the data and getting familiar with it.  Of course, given a focused business question, the same tools (and many more) can be applied in the same way.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
