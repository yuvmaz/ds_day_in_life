{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In this notebook, we'll take a look at one of the simplest and most useful algorithms in Machine Learning:  *linear regression*.  Let's look at the name:\n",
    "\n",
    "* `Linear` - the algorithm assumes that the relationship between the *independent* variables (predictors, features) and the *dependent* variable (response, target) is linear in nature.  This means that the formula for the relationship is of the form $$ y = \\alpha + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\ldots + \\beta_n \\cdot x_n \\epsilon$$\n",
    "\n",
    "where $\\alpha$ is the *intercept*, or where the line meets the $y$-axis (i.e., $x = 0$) and $\\beta_i$ is the *coefficient* for feature $i$.  $\\epsilon$ is the *noise* - we assume that our data is not perfect and there is some random variation we cannot - and should not - model.  In other words, the relationship is a *sum* of *products* of constants and a variable.  In fact, if we define a feature $x_0$ with a constant value of 1 and write $\\alpha$ as $\\beta_0$ we can write $$ y = 1 \\cdot \\alpha + \\sum_{i=1}^{n} \\beta_i \\cdot x_i + \\epsilon = \\sum_{i=0}^{n} \\beta_i \\cdot x_i + \\epsilon $$\n",
    "\n",
    "* `Regression` - the algorithm looks for the *best fit* line:  the line that minimizes the *Mean Squared Error* (MSE) between the points and the prediction.  Formally:  assuming that $\\hat{y}$ is the predicted value for the input $x$, the algorithm tries to minimize $ (\\hat{y} - y)^2 $ for all values of $\\hat{y}$.\n",
    "\n",
    "Linear regression has some assumptions - \n",
    "1. The relationship between $x$ and $y$ is linear (obviously)\n",
    "2. The values are independent - i.e., having seen the value $a$ does not affect the chances of seeing the value $b$\n",
    "3.  The error is constant across all $x$'s\n",
    "4.  Errors are normally distributed\n",
    "\n",
    "\n",
    "Let's see an example.\n",
    "\n",
    "## Example\n",
    "\n",
    "### The Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "\n",
    "rcParams['figure.figsize'] = (20,7)\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with features\n",
    "df = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
    "\n",
    "# Add target\n",
    "df['MEDV'] = boston['target']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new dataset, we usually begin by doing *exploratory data analysis* and getting to know the data as well as  cleaning up the data.  Here, since this is a well-known clean dataset we'll dispense with that.  \n",
    "\n",
    "Only thing we need to check is assumption 4 above - that the error is normally distributed.  In practice this means that we expect to see something resembling a bell curve when we plot the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect, but it will do.  Usually linear regression tends to work with 'normal-like' distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a *predictive* model - one that learns parameters and which we can use for new data.  Alternatively, we can build an *inferential* model, which can help us explain how the various features affect the target value.\n",
    "\n",
    "In order to be able to measure the quality of our model we'll split the data into a *training* and *test* set:  we'll learn on the training set and check error - MSE - on the test set.  (Strictly speaking, we'll also need a *validation* set for comparing models with different features - but since we have little data we'll solve this another way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df[df.columns[-1]])\n",
    "print(\"Training set size: {} {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test set size: {} {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a single predictor first - the average number of rooms.  How does this feature predict the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's always good to get intuition with a plot!\n",
    "scatter(X_train['RM'], y_train)\n",
    "xlabel(\"Avg. # of Rooms\")\n",
    "ylabel(\"Median house price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see something that looks linear.  Let's try fitting and drawing the resulting line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the object\n",
    "lr_rooms = LinearRegression()\n",
    "\n",
    "# Fit X to y values\n",
    "lr_rooms.fit(X_train[['RM']], y_train)\n",
    "\n",
    "# Get model predictions\n",
    "predictions = lr_rooms.predict(X_train[['RM']])\n",
    "\n",
    "scatter(X_train['RM'], y_train)\n",
    "plot(X_train['RM'], predictions)\n",
    "xlabel(\"Avg. # of Rooms\")\n",
    "ylabel(\"Median house price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model looks reasonably good, however it tends to underestimate price at the lower end (3 - 5 rooms on average) and at the upper end (7.5 - 9 rooms on average).  Let's quantify this by looking at the MSE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the error between the true values and the predicted values\n",
    "mse = mean_squared_error(y_train, predictions)\n",
    "\n",
    "print(\"MSE: {}\".format(mse))\n",
    "print(\"We miss the target by {:.4}K on average\".format(sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this MSE value is for the *training* set.  We're interested in what happens with unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_rooms.predict(X_test[['RM']])\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "print(\"MSE: {}\".format(mse))\n",
    "print(\"We miss the target by {:.4}K on average\".format(sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEATURE\\tMSE\")\n",
    "print(\"-------\\t---\")\n",
    "\n",
    "for feature in ['AGE','DIS','CRIM']:\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train[[feature]], y_train)\n",
    "    test_predictions = lr.predict(X_test[[feature]])\n",
    "    mse = mean_squared_error(y_test, test_predictions)\n",
    "    print(\"{}\\t{:.5}\".format(feature, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been working with a single feature to predict the target.  What happens when we use more than one feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two features\n",
    "lr_room_age = LinearRegression()\n",
    "lr_room_age.fit(X_train[['RM','AGE']], y_train)\n",
    "lr_room_age_test_predictions = lr_room_age.predict(X_test[['RM','AGE']].values)\n",
    "mse = mean_squared_error(y_test, lr_room_age_test_predictions)\n",
    "print(\"MSE: {:.5}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three features\n",
    "lr_room_age_dist = LinearRegression()\n",
    "lr_room_age_dist.fit(X_train[['RM','AGE','DIS']], y_train)\n",
    "lr_room_age_dist_test_predictions = lr_room_age_dist.predict(X_test[['RM','AGE','DIS']].values)\n",
    "mse = mean_squared_error(y_test, lr_room_age_dist_test_predictions)\n",
    "print(\"MSE: {:.5}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In going from 2 to 3 features we get a better MSE score - but it seems a very small improvement.  How can we tell how 'good' our fit is?  \n",
    "\n",
    "We use a statistical measure called $R^2$ (pronounced R-squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"R2 for two features  : {:.5}\".format(r2_score(lr_room_age_test_predictions, y_test)))\n",
    "print(\"R2 for three features: {:.5}\".format(r2_score(lr_room_age_dist_test_predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the 3-feature model is actually *worse* than the 2-feature model from a statistical point of view.  In practice, it delivers a lower MSE.  Which is better depends on the business problem.\n",
    "\n",
    "Small note:  adding more parameters to a model will always decrease its MSE.  The reason is that the model is more complex and hence can accomodate the data better.  However, we must beware of *overfitting*, where we treat the noise as part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have a validation dataset, we're using our test set to check what features increase or $R^2$ and/or lower our MSE.  But in doing so we might be overfitting to the specific test set.  On the other hand, we don't have a lot of data so we need everything we can get for reasonably-sized train and test sets.  What to do?\n",
    "\n",
    "The solution is called *k-fold cross validation*.  We split our data into $k$ parts (or *folds*).  We train on $k-1$ folds and test on the last remaining fold.  We repeat this procedure $k$ times - in other words, each fold gets the chance to be a 'test set' of sorts.  By doing this we ensure that we use all of the available data for both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def score_fold(trained_lr,X,y):\n",
    "    predictions = trained_lr.predict(X)\n",
    "    return mean_squared_error(y, predictions)\n",
    "\n",
    "# Use default 5-fold CV\n",
    "cross_val_score(LinearRegression(), df[['RM']], df['MEDV'], scoring=score_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a model with the single RM feature does not return similar MSE values for all folds.  So back to the drawing board... \n",
    "\n",
    "Once we find a good model we train it with *all* the data and deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression \n",
    "\n",
    "As we saw above, the form of our relationship is linear:  $ \\alpha + \\beta_1 \\cdot x_1 + \\ldots + \\beta_n \\cdot x_n $.  Although each $\\beta$ is a constant, there is no limitation on the $x$'s themselves.  So it's perfectly legal to have a *linear* relationship such as this: $$ \\alpha + \\beta_1 \\cdot x + \\beta_2 \\cdot x^2 $$\n",
    "\n",
    "This is known as *polynomial regression*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_train[['RM']]\n",
    "data['RM2'] = data['RM'] ** 2\n",
    "\n",
    "lr_room_room2 = LinearRegression()\n",
    "lr_room_room2.fit(data, y_train)\n",
    "predictions = lr_room_room2.predict(data)\n",
    "\n",
    "scatter(X_train['RM'], y_train)\n",
    "scatter(X_train['RM'], predictions)\n",
    "xlabel(\"Avg. # of Rooms\")\n",
    "ylabel(\"Median house price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that using the squared number of rooms leads to a model that better captures the shape of the data.  Let's quantify this intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = X_test[['RM']]\n",
    "test_data['RM2'] = test_data['RM'] ** 2\n",
    "\n",
    "test_predictions = lr_room_room2.predict(test_data)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"MSE: {:.5}\".format(test_mse))\n",
    "print(\"R2: {:.5}\".format(test_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersting - it looks as if the *squared* average number of rooms in the house can significantly increase $R^2$ and lower MSE.  This is good news for the business - our predictive model makes smaller mistakes.  But now the question arises - can we understand the influence of both the regular and squared values on the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Linear Regression Coefficients\n",
    "As it turns out, this is definitely possible.  In fact, the *coefficients* of the $x$'s (what we called $\\beta_1, \\beta_2$, etc.) are exactly what we need.  Each coefficient in a linear regression tells us buy how much the target changes when we increase the relevant variable by 1 unit and *keep all other variables constant*.  Let's see this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the coefficients for x and x^2\n",
    "\n",
    "\n",
    "lr_room_room2.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers tell us that when we increase $x^2$ by 1, the target increases by a positive amount.  It also tells us that when we increase $x$ by 1, the target increases by a negative amount (i.e., decreases).  Say what???  How can increasing the number of average rooms in the house *reduce* the house price???  \n",
    "\n",
    "The reason is that $x$ and $x^2$ are *correlated*.  In other words, increasing or decreasing one will increase or decrease the other one.  In linear regression, this means that they explain the same thing.  Normally, this is a problem.  Here, because our two variables are powers of each other, we can continue safely.  Because $x^2$ is bigger than $x$, we see that it does most of the *work* and is positive as we expect.  The regular $x$ makes sure we don't increase the target too much.\n",
    "\n",
    "We'll now try to give our model some values and these increases/decreases in practice.  Since we'll change $x$ without changing $x^2$ (or vice-versa) this is called a *counter-factual*, as we don't have real data for this case (nor will we ever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_one = lr_room_room2.predict([[1,1]])[0]\n",
    "two_one = lr_room_room2.predict([[2,1]])[0]\n",
    "one_two = lr_room_room2.predict([[1,2]])[0]\n",
    "\n",
    "print(\"model(2,1) - model(1,1) = {:.5}, coef(x) = {:.5}\".format(\n",
    "    (two_one - one_one), lr_room_room2.coef_[0]))\n",
    "\n",
    "print(\"model(1,2) - model(1,1) = {:.5}, coef(x^2) = {:.5}\".format(\n",
    "    (one_two - one_one), lr_room_room2.coef_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we introuced the *linear regression* algorithm for finding a best linear fit that minimizes the MSE between a data point and its prediction.  We further saw that a linear relationship can also apply to functions of the original variables - such as powers.  Finally, we took a short look at the interpretation of the coefficients of the fit.  In the process we also introduced the use of k-fold cross-validation to make sure that our models can generalize.  \n",
    "\n",
    "Linear regression is one of the main workhorses of the data science world - it's practically everywhere.  Because it's so common, many innovations have been proposed that improve performance and accuracy or deal with different kinds of error distributions.  Regardless, you'll find the linear regression useful in practically everything you do.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1.  Try to find the optimal combination of features on the Boston Housing dataset that minimizes MSE.  Does this minimal MSE also correspond to a maximal $R^2$?  \n",
    "\n",
    "FYI: The maximum possible value of the $R^2$ metric is 1.  \n",
    "2.  Repeat the analysis on the `diabetes` dataset which you can access using the `load_diabetes` function in sklearn.datasets.  \n",
    "\n",
    "    a.  What is the minimum MSE/maximum $R^2$ you can reach?\n",
    "    \n",
    "    b.  What can you say about each feature?  Does it increase or decrease the progression of the disease?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
