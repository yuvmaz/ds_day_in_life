{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees & Random Forests\n",
    "\n",
    "In this notebook, we'll take a look at one of the most common classification algorithms:  *Random Forests*.  But in order to understand it, we need to start with a different algorithm called a *Decision Tree*.  Let's look at the name:\n",
    "\n",
    "* `Tree` - The algorithm takes a look at all the features in the dataset, and creates a *tree* - very similiar to a flowchart.  At each internal node in the tree there is the name of a feature.  The leaves are labeled with a class:\n",
    "\n",
    "![image](https://miro.medium.com/max/1680/0*DKVni_-q7dAKVel7.png)\n",
    "\n",
    "* `Decision` - The algorithm starts at the top of the tree, and looks at feature name.  Based on the value of the feature, it advances to one of its children.  It then repeats the process until it reaches a leaf node.  This leaf node determines the class.  \n",
    "\n",
    "What we would like the algorithm to do is to figure out which features need to be at what level of the tree, as well as what the values should be that lead to each child.\n",
    "\n",
    "Interestingly, the decision tree makes very little assumptions about its data - primarily, it expects the data to be categorical (i.e., one of $n$ possible values).  If the data is instead numerical, it is first discretized into as many groups as needed for the tree.\n",
    "\n",
    "Let's see an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "### The Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "rcParams['figure.figsize'] = (20,7)\n",
    "\n",
    "\n",
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with features\n",
    "df = pd.DataFrame(wine['data'], columns=wine['feature_names'])\n",
    "\n",
    "# Add target\n",
    "df['class'] = wine['target']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(wine['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen this dataset in the last lesson.  This time around, we'll try to use its features to predict which of 3 *classes* each wine belongs to.  (For example, a class decided by a human expert).  As always, we begin by looking at the data.  In this case, we'll let's see how many instances we have of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts().sort_index().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a fairly similiar number of instances of each class.  This is what is known as a *balanced* data set - where one class does not completely overwhelm other classes.  Many real-world datasets are *imbalanced* - in those cases, special care needs to be taken.\n",
    "\n",
    "### Setting Up the Experiment\n",
    "\n",
    "As this is a *supervised* problem, we can split the data into train and test sets.  Note, however, that we need to make sure that both sets have the same distribution of classes - otherwise, we might only see a certain class in the training or in the test sets.  We call this a *stratified* split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df[df.columns[-1]], stratify=df['class'],\n",
    "                                                   random_state=234234)\n",
    "print(\"Training set size: {} {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test set size: {} {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set label distribution:\")\n",
    "print(y_train.value_counts().sort_index() / y_train.sum())\n",
    "\n",
    "print(\"Test set label distribution:\")\n",
    "print(y_test.value_counts().sort_index() / y_test.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK then, percentages seem to be similiar.  We can hope that our model will generalize properly.  Let's go ahead and try to fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on train set:\")\n",
    "print(accuracy_score(y_train, tree.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do test set results look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set:\")\n",
    "predictions = tree.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad score!  But since the dataset is not *perfectly* balanced let's take a look at a more detailed form of errors.  This is called a *confusion matrix*:  values in *rows* are the *actual* (true) values, while values in *columns* are *predicted* values.  Let's look at the confusion matrix for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So cell (0,0) (top-left corner) means we have 14 values that were correctly identified as class_0.  Cell (0,1) means that 1 class_0 value was mis-classified as class_1, and cell (0,2) means that no class_0 values were misclassified as class_2.  So basically, the bigger the diagonal of the confusion matrix is with regards to the non-diagonal values, the better.  Let's see this in percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm / cm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now in graphical form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(cm / cm.sum(axis=1), extent=None)\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Metrics\n",
    "\n",
    "From the above confusion matrix we can see that there 2 forms of possible errors for a single datum:  either it was supposed to be class $i$ but was classified as class $j$ - a *false negative* - or it was supposed to be class $j$ but was classified as class $i$ - a *false positive*.  We would like to know what kind of mistakes our model makes, as often real-world costs differ between false negatives and false positives.\n",
    "\n",
    "The `precision` is a measure of false positives.  Its formula:\n",
    "\n",
    "$$ precision = \\frac{tp}{tp+fp} $$\n",
    "\n",
    "The true positives is the number in cell $(i,i)$ of the confusion matrix.  False positives are items *predicted* to be class $i$, but are *actually* class $j \\ne i$ - so they are in *column* $i$ but *rows* different from $i$.  In our case, for class 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm[0,0] / (cm[0,0] + cm[1,0] + cm[2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, precision for class_0 in our model is 0.875 (or 0.88, rounded to 2 decimal points).  In a similiar vein, we'd like to measure the tendency for false negatives.  This is `recall`:\n",
    "\n",
    "$$ recall = \\frac{tp}{tp+fn} $$\n",
    "\n",
    "False negatives are items that are *actually* class $i$ but *predicted* to be class $j \\ne i$ - so they are in *row* $i$ but *columns* different to $i.  In our case for class 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm[0,0] / (cm[0,0] + cm[0,1] + cm[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So recall for class_0 in our model is 0.9333 (or 0.93, rounded to 2 decimal points). Obviously, we don't need to calculate this on our own: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is the *harmonic mean* of precision and recall:\n",
    "\n",
    "$$ F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Random Forest Algorithm\n",
    "\n",
    "If you recall, we saw in our earlier lessons that a single train-test split might not be representative and in this way lead to overfitting.  We also saw the technique of *k-fold cross-validation* that makes sure every individual piece of data appears once in the test set and $k-1$ time in the train set, for a given $k$.  \n",
    "\n",
    "The *Random Forests* algorithm does something similiar for decision trees.  Rather than use a single decision tree and (over)-fit it to specific data, it fits *many* trees to different subsets of both data *and* features.  This idea is called *bagging* and its purpose is to provide many *weak* models (i.e., trained on only partial data) that can be combined into a *strong* model.  I.e., for every new piece of data, each one of those $n$ models can give its 'opinion' and the final result is determined by a majority vote.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_train, random_forest.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, random_forest.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a marked improvement from the single-tree F1 score!  Let's see how far this idea can take us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "for num_trees in range(1,100):\n",
    "    forest = RandomForestClassifier(n_estimators=num_trees, random_state=65465)\n",
    "    forest.fit(X_train, y_train)\n",
    "    scores.append(f1_score(y_test, forest.predict(X_test), average=\"weighted\"))\n",
    "    \n",
    "\n",
    "plot(range(1,100), scores)\n",
    "grid('both')\n",
    "xlabel(\"# of Tress in Random Forest\")\n",
    "ylabel(\"$F_1$ score\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we saw two variants of decision trees - a single tree trained on all data and features, and a forest of trees trained on *random* subsets of data and features.  We discussed the concepts of precision and recall and used them to measure the tendency of our model to produce false negatives and false positives.  Finally, we saw that even a relatively small random forest can already lead to good models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  From the Machine Learning Repository at the University of California at Irvine (https://archive.ics.uci.edu/ml/datasets) choose any dataset that has the classification task.  What is F1 score for a single decision tree?  For a random forest with n=10 trees?  What is the optimal number of trees for the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
